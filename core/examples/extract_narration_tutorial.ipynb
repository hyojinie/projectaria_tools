{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb140897",
      "metadata": {
        "id": "fb140897"
      },
      "source": [
        "# Extract Narrations and Audio Captions Tutorial\n",
        "\n",
        "In this tutorial, you will be extracting video narrations through an auto-narration model, LaViLa, as well as audio captions through speech-to-text model, WhisperX. Finally, you will be able to interact with the extracted narrations and captions using langchain.\n",
        "\n",
        "### Notebook stuck?\n",
        "Note that because of Jupyter issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Install Project Aria Tools\n",
        "Run the following cell to install Project Aria Tools for reading Aria recordings in .vrs format"
      ],
      "metadata": {
        "id": "l5LlycOs75bf"
      },
      "id": "l5LlycOs75bf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifics for Google Colab\n",
        "google_colab_env = 'google.colab' in str(get_ipython())\n",
        "print(\"Running from Google Colab, installing projectaria_tools\")\n",
        "!pip install projectaria-tools"
      ],
      "metadata": {
        "id": "Yt5CoQlp8Cxw"
      },
      "id": "Yt5CoQlp8Cxw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Prepare an Aria recording\n",
        "\n",
        "#### (Option 1) Download a sample data\n",
        "We recommend running this tutorial with this small scale sample data first for testing out the dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tv0I3ajm7TyH"
      },
      "id": "Tv0I3ajm7TyH"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O -J -L \"https://github.com/facebookresearch/projectaria_tools/raw/main/data/mps_sample/sample.vrs\"\n",
        "vrsfile = \"sample.vrs\"\n",
        "print(f\"INFO: vrsfile set to {vrsfile}\")"
      ],
      "metadata": {
        "id": "WKyjF8fF7tA_"
      },
      "id": "WKyjF8fF7tA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Option 2) Prepare your collected Aria recording\n",
        "We will set the vrsfile path to your collected Aria recording.\n",
        "\n",
        "Upload your Aria recording in your Google Drive before running the cell.\n",
        "\n",
        "Here, we assume it is uploaded to **`My Drive/aria/recording.vrs`**\n",
        "\n",
        "*(You can check the content of the mounted drive by running `!ls \"/content/drive/My Drive/\"` in a cell.)*"
      ],
      "metadata": {
        "id": "vZLboooc7QLQ"
      },
      "id": "vZLboooc7QLQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive/')\n",
        "my_vrs_file_path = 'aria/recording.vrs'\n",
        "vrsfile = \"/content/drive/My Drive/\" + my_vrs_file_path\n",
        "print(f\"INFO: vrsfile set to {vrsfile}\")"
      ],
      "metadata": {
        "id": "vnjjSGXlc4-i"
      },
      "id": "vnjjSGXlc4-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8196ad05",
      "metadata": {
        "id": "8196ad05"
      },
      "source": [
        "## Step 3. Create data provider\n",
        "\n",
        "Create projectaria data_provider so you can load the content of the vrs file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb04b53b",
      "metadata": {
        "id": "fb04b53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72defac0-8eae-41be-afc4-44f132e39e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data provider from /content/drive/My Drive/aria/recording_long.vrs\n"
          ]
        }
      ],
      "source": [
        "from projectaria_tools.core import data_provider, calibration\n",
        "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
        "from projectaria_tools.core.stream_id import RecordableTypeId, StreamId\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(f\"Creating data provider from {vrsfile}\")\n",
        "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
        "if not provider:\n",
        "    print(\"Invalid vrs data provider\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5033225",
      "metadata": {
        "id": "c5033225"
      },
      "source": [
        "## Step 4. Display VRS rgb content in thumbnail images\n",
        "\n",
        "Goals:\n",
        "- Summarize a VRS using 10 image side by side, to visually inspect the collected data.\n",
        "\n",
        "Key learnings:\n",
        "- Image streams are identified with a Unique Identifier: stream_id\n",
        "- Image frames are identified with timestamps\n",
        "- PIL images can be created from Numpy array\n",
        "\n",
        "Customization\n",
        "- To change the number of sampled images, change the variable `sample_count` to a desired number.\n",
        "- To change the thumbnail size, change the variable `resize_ratio` to a desired value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933725b6",
      "metadata": {
        "id": "933725b6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "sample_count = 10\n",
        "resize_ratio = 10\n",
        "\n",
        "rgb_stream_id = StreamId(\"214-1\")\n",
        "\n",
        "# Retrieve image size for the RGB stream\n",
        "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
        "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
        "\n",
        "# Retrieve Start and End time for the given Sensor Stream Id\n",
        "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
        "end_time = provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
        "\n",
        "image_config = provider.get_image_configuration(rgb_stream_id)\n",
        "width = image_config.image_width\n",
        "height = image_config.image_height\n",
        "\n",
        "thumbnail = newImage = Image.new(\n",
        "    \"RGB\", (int(width * sample_count / resize_ratio), int(height / resize_ratio))\n",
        ")\n",
        "current_width = 0\n",
        "\n",
        "\n",
        "# Samples 10 timestamps\n",
        "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
        "for sample in tqdm(sample_timestamps):\n",
        "    image_tuple = provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
        "    image_array = image_tuple[0].to_numpy_array()\n",
        "    image = Image.fromarray(image_array)\n",
        "    new_size = (\n",
        "        int(image.size[0] / resize_ratio),\n",
        "        int(image.size[1] / resize_ratio),\n",
        "    )\n",
        "    image = image.resize(new_size).rotate(-90)\n",
        "    thumbnail.paste(image, (current_width, 0))\n",
        "    current_width = int(current_width + width / resize_ratio)\n",
        "\n",
        "from IPython.display import Image\n",
        "display(thumbnail)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Prepare Pytorch Data Loader for Auto-Narration\n",
        "\n",
        "Here, we will be creating a pytorch data loader that outputs batches of video snippets in order to run the LaViLa auto-narration model.\n",
        "\n",
        "A **snippet** consists of a series of frames captured over a brief time span, which we will refer to as **snippet duration**.\n",
        "\n",
        "#### Step 5-1. Define Dataset"
      ],
      "metadata": {
        "id": "nFkss4vlaWxh"
      },
      "id": "nFkss4vlaWxh"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms._transforms_video as transforms_video\n",
        "import torch.nn as nn\n",
        "\n",
        "class RGBSnippetDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 start_time: float, # start time in the video for sampling data\n",
        "                 end_time: float, # end time in the video for sampling data\n",
        "                 snippet_dur_sec: int, # snippet duration\n",
        "                 frames_per_snippet: int, # number of frames per snippet\n",
        "                 transform=None\n",
        "    ):\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "        self.snippet_dur = snippet_dur_sec * 1000000000 # duration of a snippet in nano seconds\n",
        "        self.frames_per_snippet = frames_per_snippet # number of frames per snippet\n",
        "        self.stride_ns = int(self.snippet_dur//frames_per_snippet)\n",
        "        self.num_snippets = int((end_time - start_time) // self.snippet_dur)\n",
        "        self.snippet_starts = np.arange(start_time, start_time + self.snippet_dur * self.num_snippets, self.snippet_dur)\n",
        "\n",
        "        # Precompute timestamps for each snippet\n",
        "        self.all_frame_timestamps = [np.arange(snippet_start, snippet_start + self.snippet_dur, self.stride_ns) for snippet_start in self.snippet_starts]\n",
        "\n",
        "        self.rgb_stream_id = rgb_stream_id\n",
        "        self.time_domain = time_domain\n",
        "        self.option = option\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_snippets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns a snippet\n",
        "\n",
        "        # get timestamps of frames that belong to the current snippet idx\n",
        "        frame_timestamps = self.all_frame_timestamps[idx]\n",
        "\n",
        "        # read frames from the data provider and append to frame_list\n",
        "        frame_list = []\n",
        "        for timestamp in frame_timestamps:\n",
        "            image_tuple = provider.get_image_data_by_time_ns(self.rgb_stream_id, int(timestamp), self.time_domain, self.option)\n",
        "            image_array = image_tuple[0].to_numpy_array()\n",
        "            frame_list.append(image_array)\n",
        "\n",
        "        # append a set of images to a snippet\n",
        "        frames = [torch.tensor(frame, dtype=torch.float32) for frame in frame_list]\n",
        "        frames = torch.stack(frames, dim=0)\n",
        "\n",
        "        if self.transform:\n",
        "          frames = self.transform(frames)\n",
        "\n",
        "        # return snippet start time and end time\n",
        "        snippet_start = self.snippet_starts[idx]\n",
        "        snippet_end = snippet_start + self.snippet_dur\n",
        "\n",
        "        return frames, snippet_start, snippet_end\n",
        "\n",
        "class Permute(nn.Module):\n",
        "    \"\"\"\n",
        "    Permutation as an op\n",
        "    \"\"\"\n",
        "    def __init__(self, ordering):\n",
        "        super().__init__()\n",
        "        self.ordering = ordering\n",
        "\n",
        "    def forward(self, frames):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            frames in some ordering, by default (C, T, H, W)\n",
        "        Returns:\n",
        "            frames in the ordering that was specified\n",
        "        \"\"\"\n",
        "        return frames.permute(self.ordering)"
      ],
      "metadata": {
        "id": "Kee0HuCgKUy_"
      },
      "id": "Kee0HuCgKUy_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5-2. Construct Data Loader\n",
        "Here you can set batch size (`batch_size`) as well as customize start time, end_time for running auto-narration."
      ],
      "metadata": {
        "id": "N7p7uCJZKPNm"
      },
      "id": "N7p7uCJZKPNm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Start and End time for the given Sensor Stream Id\n",
        "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
        "end_time = provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
        "\n",
        "batch_size = 2 # batch size in dataloader (Decrease/increase based on the GPU memory)\n",
        "image_size = 224  # image size after resizing (Do not change for LaViLa)\n",
        "snippet_dur_sec = 2  # duration of a snippet (We recommend values between 1-10.)\n",
        "frames_per_snippet = 4  # number of frames per snippet (Do not change for LaViLa)\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    Permute([3, 0, 1, 2]),  # T H W C -> C T H W\n",
        "    transforms.Resize(image_size),\n",
        "    transforms_video.NormalizeVideo(mean=[108.3272985, 116.7460125, 104.09373615000001], std=[68.5005327, 66.6321579, 70.32316305]),\n",
        "])\n",
        "rgb_snippet_dataset = RGBSnippetDataset(start_time, end_time, snippet_dur_sec=snippet_dur_sec, frames_per_snippet=frames_per_snippet, transform=val_transform)\n",
        "snippet_dataloader = DataLoader(rgb_snippet_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "gWvmKM_nS0Ri"
      },
      "id": "gWvmKM_nS0Ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6. Install LaViLa auto-narration library\n",
        "Now that the data is prepared, let's install LaViLa library.\n",
        "\n",
        "LaViLa (Language augmented Video Language Pretraining) is a video narration model that is trained on Ego4D.\n",
        "It is used for generating text descriptions for your captured recordings in this tutorial.\n",
        "- Paper: https://arxiv.org/abs/2212.04501\n",
        "- Code: https://github.com/facebookresearch/LaViLa"
      ],
      "metadata": {
        "id": "ixrGS0J0afaE"
      },
      "id": "ixrGS0J0afaE"
    },
    {
      "cell_type": "code",
      "source": [
        "# install LaViLa as dependency\n",
        "!pip install git+https://github.com/zhaoyang-lv/LaViLa"
      ],
      "metadata": {
        "id": "7vLd1utsaqAr"
      },
      "id": "7vLd1utsaqAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. Define helper functions for LaViLa\n",
        "\n",
        "Run the following cell for defining helper functions for (1) loading pre-trained models and tokenizers, (2) decoding generated tokens, and (3) run model on a batch of snippets."
      ],
      "metadata": {
        "id": "9xPZVr6eMIsp"
      },
      "id": "9xPZVr6eMIsp"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "from lavila.models.models import VCLM_OPENAI_TIMESFORMER_LARGE_336PX_GPT2_XL, VCLM_OPENAI_TIMESFORMER_BASE_GPT2\n",
        "from lavila.models.tokenizer import MyGPT2Tokenizer\n",
        "\n",
        "DEFAULT_CHECKPOINT = 'vclm_openai_timesformer_base_gpt2_base.pt_ego4d.jobid_319630.ep_0002.md5sum_68a71f.pth'\n",
        "# DEFAULT_CHECKPOINT = 'vclm_openai_timesformer_large_336px_gpt2_xl.pt_ego4d.jobid_246897.ep_0003.md5sum_443263.pth'\n",
        "\n",
        "def load_models_and_transforms(num_frames=4, ckpt_name=DEFAULT_CHECKPOINT, device='cpu'):\n",
        "    '''\n",
        "    Helper function for loading oading pre-trained models and tokenizers\n",
        "    '''\n",
        "    ckpt_path = os.path.join('lavila/modelzoo/', ckpt_name)\n",
        "    print(f\"ckpt_path: {os.path.abspath(ckpt_path)}\")\n",
        "    os.makedirs('lavila/modelzoo/', exist_ok=True)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print('downloading model to {}'.format(ckpt_path))\n",
        "        urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/lavila/checkpoints/narrator/{}'.format(ckpt_name), ckpt_path)\n",
        "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "    state_dict = OrderedDict()\n",
        "    for k, v in ckpt['state_dict'].items():\n",
        "        state_dict[k.replace('module.', '')] = v\n",
        "\n",
        "    # instantiate the model, and load the pre-trained weights\n",
        "    # model = VCLM_OPENAI_TIMESFORMER_LARGE_336PX_GPT2_XL(\n",
        "    model = VCLM_OPENAI_TIMESFORMER_BASE_GPT2(\n",
        "        text_use_cls_token=False,\n",
        "        project_embed_dim=256,\n",
        "        gated_xattn=True,\n",
        "        timesformer_gated_xattn=False,\n",
        "        freeze_lm_vclm=False,      # we use model.eval() anyway\n",
        "        freeze_visual_vclm=False,  # we use model.eval() anyway\n",
        "        num_frames=num_frames,\n",
        "        drop_path_rate=0.\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    device_type_str = device.type if isinstance(device, torch.device) else device\n",
        "    if device_type_str != 'cpu':\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = MyGPT2Tokenizer('gpt2', add_bos=True)\n",
        "    #tokenizer = MyGPT2Tokenizer('gpt2-xl', add_bos=True)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def decode_one(generated_ids, tokenizer):\n",
        "    '''\n",
        "    Helper function for decoding generated tokens.\n",
        "    '''\n",
        "    # get the index of <EOS>\n",
        "    if tokenizer.eos_token_id == tokenizer.bos_token_id:\n",
        "        if tokenizer.eos_token_id in generated_ids[1:].tolist():\n",
        "            eos_id = generated_ids[1:].tolist().index(tokenizer.eos_token_id) + 1\n",
        "        else:\n",
        "            eos_id = len(generated_ids.tolist()) - 1\n",
        "    elif tokenizer.eos_token_id in generated_ids.tolist():\n",
        "        eos_id = generated_ids.tolist().index(tokenizer.eos_token_id)\n",
        "    else:\n",
        "        eos_id = len(generated_ids.tolist()) - 1\n",
        "    generated_text_str = tokenizer.tokenizer.decode(generated_ids[1:eos_id].tolist())\n",
        "    return generated_text_str\n",
        "\n",
        "\n",
        "def run_model_on_snippets(\n",
        "    frames, model, tokenizer, device=\"cpu\", narration_max_sentences=5\n",
        "):\n",
        "    '''\n",
        "    Function for running the LaViLa model on batches of snippets.\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(frames)\n",
        "        generated_text_ids, ppls = model.generate(\n",
        "            image_features,\n",
        "            tokenizer,\n",
        "            target=None,  # free-form generation\n",
        "            max_text_length=77,\n",
        "            top_k=None,\n",
        "            top_p=0.95,   # nucleus sampling\n",
        "            num_return_sequences=narration_max_sentences,  # number of candidates: 10\n",
        "            temperature=0.7,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    output_narration = []\n",
        "    for j in range(generated_text_ids.shape[0] // narration_max_sentences):\n",
        "        cur_output_narration = []\n",
        "        for k in range(narration_max_sentences):\n",
        "            jj = j * narration_max_sentences + k\n",
        "            generated_text_str = decode_one(generated_text_ids[jj], tokenizer)\n",
        "            generated_text_str = generated_text_str.strip()\n",
        "            generated_text_str = generated_text_str.replace(\"#c c\", \"#C C\")\n",
        "            if generated_text_str in cur_output_narration:\n",
        "                continue\n",
        "            if generated_text_str.endswith('the'):\n",
        "                # skip incomplete sentences\n",
        "                continue\n",
        "            cur_output_narration.append(generated_text_str)\n",
        "        output_narration.append(cur_output_narration) # list of size B (batch size)\n",
        "    return output_narration"
      ],
      "metadata": {
        "id": "qFnuvPo1b3Dk"
      },
      "id": "qFnuvPo1b3Dk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8. Run LaViLa inference over vrs file\n",
        "Let's load the pre-traiend model and tokenizer\n"
      ],
      "metadata": {
        "id": "mJ7qlnBevC06"
      },
      "id": "mJ7qlnBevC06"
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pre-traiend model and tokenizer.\n",
        "model, tokenizer = load_models_and_transforms(num_frames=4)\n",
        "\n",
        "# this is where the generated narration will be stored\n",
        "narrations_dict = {\n",
        "    'start_time_ns': [],\n",
        "    'end_time_ns': [],\n",
        "    'narration': [],\n",
        "}\n",
        "\n",
        "# use gpu if available\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "for idx, (frames, st_ns, ed_ns) in enumerate(snippet_dataloader):\n",
        "  if torch.cuda.is_available():\n",
        "    frames = frames.cuda()\n",
        "  # run inference over a batch of snippet\n",
        "  output_narration = run_model_on_snippets(frames, model, tokenizer)\n",
        "  # store results\n",
        "  narrations_dict['start_time_ns'].extend(st_ns.numpy().tolist())\n",
        "  narrations_dict['end_time_ns'].extend(ed_ns.numpy().tolist())\n",
        "  narrations_dict['narration'].extend(output_narration)"
      ],
      "metadata": {
        "id": "d7C99NViqb_a"
      },
      "id": "d7C99NViqb_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9. Display the auto-narration results and save to csv file\n",
        "Make sure to change `narration_save_path` to your desired location!"
      ],
      "metadata": {
        "id": "z3pRVNe5vPIL"
      },
      "id": "z3pRVNe5vPIL"
    },
    {
      "cell_type": "code",
      "source": [
        "narration_save_path = os.path.join(os.path.dirname(vrsfile), 'auto_narration.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(narrations_dict)\n",
        "df.to_csv(narration_save_path)\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "E6nUZYKurDpO",
        "outputId": "8164a676-94d8-4c90-8d34-2b9674d60947"
      },
      "id": "E6nUZYKurDpO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   start_time_ns   end_time_ns  \\\n",
              "0   148502526450  150502526450   \n",
              "1   150502526450  152502526450   \n",
              "2   152502526450  154502526450   \n",
              "3   154502526450  156502526450   \n",
              "4   156502526450  158502526450   \n",
              "5   158502526450  160502526450   \n",
              "\n",
              "                                           narration  \n",
              "0  [#C C stares at the ceiling, #C C looks around...  \n",
              "1  [#C C looks around the house, #C C looks aroun...  \n",
              "2       [#C C adjusts the camera, #C C looks around]  \n",
              "3                                [#C C looks around]  \n",
              "4   [#C C stands beside the door, #C C looks around]  \n",
              "5  [#C C looks at the wall, #C C looks around, #C...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time_ns</th>\n",
              "      <th>end_time_ns</th>\n",
              "      <th>narration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148502526450</td>\n",
              "      <td>150502526450</td>\n",
              "      <td>[#C C stares at the ceiling, #C C looks around...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150502526450</td>\n",
              "      <td>152502526450</td>\n",
              "      <td>[#C C looks around the house, #C C looks aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>152502526450</td>\n",
              "      <td>154502526450</td>\n",
              "      <td>[#C C adjusts the camera, #C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154502526450</td>\n",
              "      <td>156502526450</td>\n",
              "      <td>[#C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156502526450</td>\n",
              "      <td>158502526450</td>\n",
              "      <td>[#C C stands beside the door, #C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>158502526450</td>\n",
              "      <td>160502526450</td>\n",
              "      <td>[#C C looks at the wall, #C C looks around, #C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9b5fcf7-00b2-428b-b18a-610e592c61d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9b5fcf7-00b2-428b-b18a-610e592c61d4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9b5fcf7-00b2-428b-b18a-610e592c61d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Steps for Speech2Text\n",
        "Proceed Step 10-14, if you have speech in your recording and would like to use it."
      ],
      "metadata": {
        "id": "BjF3Ms0l3igG"
      },
      "id": "BjF3Ms0l3igG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10. Build VRS Tool to extract .wav file for audio captioning\n",
        "Whisper X can be run on .wav file. We need to install VRSTool for extracting .wav file from .vrs file."
      ],
      "metadata": {
        "id": "upn5Is4AhRpm"
      },
      "id": "upn5Is4AhRpm"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "# Install VRS dependencies\n",
        "!sudo apt-get install cmake git ninja-build ccache libgtest-dev libfmt-dev libturbojpeg-dev libpng-dev\n",
        "!sudo apt-get install liblz4-dev libzstd-dev libxxhash-dev\n",
        "!sudo apt-get install libboost-system-dev libboost-filesystem-dev libboost-thread-dev libboost-chrono-dev libboost-date-time-dev\n",
        "# Install build dependencies\n",
        "!sudo apt-get install -y cmake ninja-build\n",
        "\n",
        "#clone and build\n",
        "!git clone https://github.com/facebookresearch/vrs.git\n",
        "!cmake -S vrs -B vrs/build -G Ninja\n",
        "!cd vrs/build; ninja vrs\n"
      ],
      "metadata": {
        "id": "N5gLce1Py09s"
      },
      "id": "N5gLce1Py09s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11. Extract .wav file from VRS file\n",
        "Now that VRSTool is installed, let's extract .wav file from .vrs file.\n",
        "\n",
        "Here, the extracted .wav file is saved to the current working directory.\n",
        "\n",
        "If you anticipate this file to be re-used, change the output path using the argument `--to <google_drive_path>`\n",
        "\n",
        "*(Ignore error '[AudioExtractor][ERROR]: os::makeDirectories(folderPath_) failed: 22, Invalid argument')*"
      ],
      "metadata": {
        "id": "jXN0HNZ5iGOy"
      },
      "id": "jXN0HNZ5iGOy"
    },
    {
      "cell_type": "code",
      "source": [
        "!./vrs/build/tools/vrs/vrs extract-audio \"{vrsfile}\" --to ."
      ],
      "metadata": {
        "id": "lSSdMm3Tz31_"
      },
      "id": "lSSdMm3Tz31_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12. Install Whisper X\n",
        "We have input data ready for Whisper X. Let's install the library.\n",
        "\n",
        "Whisper X is an automatic speech recognition method that provides word-level timestamps and speaker diarization.\n",
        "- Paper: https://arxiv.org/abs/2303.00747\n",
        "- Code: https://github.com/m-bain/whisperX"
      ],
      "metadata": {
        "id": "Wleqb4JLkNNE"
      },
      "id": "Wleqb4JLkNNE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "id": "0Su4GYTFkPKU"
      },
      "id": "0Su4GYTFkPKU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13. Define helper functions for Whisper X\n",
        "Let's define some helper functions for Whisper X, this include a postprocessing function and a function to align the output to the timestamps."
      ],
      "metadata": {
        "id": "1a2IoR3CiMxl"
      },
      "id": "1a2IoR3CiMxl"
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisperx\n",
        "import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def asr_tokens_to_csv(\n",
        "    word_segments,\n",
        "    token_csv_folder: str,\n",
        "    starting_timestamp_s: float = 0.0,\n",
        "):\n",
        "    # post process the output asr file to extract only the minimal needed content\n",
        "\n",
        "    df = pd.DataFrame.from_dict(word_segments)\n",
        "    os.makedirs(token_csv_folder, exist_ok=True)\n",
        "\n",
        "    # write to wav domain:\n",
        "    s_to_ms = int(1e3)\n",
        "    df = df.fillna(-1)\n",
        "    df[\"start\"] = (df[\"start\"] * s_to_ms).astype(\"int64\")\n",
        "    df[\"end\"] = (df[\"end\"] * s_to_ms).astype(\"int64\")\n",
        "    df_speech_wav = df.rename(\n",
        "        columns={\"start\": \"startTime_ms\", \"end\": \"endTime_ms\", \"text\": \"written\"},\n",
        "    )\n",
        "    df_speech_wav.to_csv(\n",
        "        osp.join(token_csv_folder, \"speech.csv\"), index=False, header=True\n",
        "    )\n",
        "\n",
        "    # Update ASR ms time to Aria ns time\n",
        "    s_to_ns = int(1e9)\n",
        "    ms_to_ns = int(1e6)\n",
        "    df[\"start\"] = (df[\"start\"] * ms_to_ns + starting_timestamp_s * s_to_ns).astype(\n",
        "        \"int64\"\n",
        "    )\n",
        "    df[\"end\"] = (df[\"end\"] * ms_to_ns + starting_timestamp_s * s_to_ns).astype(\"int64\")\n",
        "\n",
        "    df_aria_domain = df.rename(\n",
        "        columns={\"start\": \"startTime_ms\", \"end\": \"endTime_ms\", \"text\": \"written\"},\n",
        "    )\n",
        "    df_aria_domain.to_csv(\n",
        "        osp.join(token_csv_folder, \"speech_aria_domain.csv\"), index=False, header=True\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Generate speech.csv & speech_aria_domain.csv to {token_csv_folder}\")\n",
        "\n",
        "\n",
        "def run_whisperx_aria_wav(\n",
        "    model,\n",
        "    file_path: str,\n",
        "    output_folder: str = \"\",\n",
        "    batch_size = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run whisperx model on .wav file extracted from VRS file\n",
        "    \"\"\"\n",
        "    starting_timestamp = file_path.split(\"-\")[-1].replace(\".wav\", \"\")\n",
        "    starting_timestamp = float(starting_timestamp)\n",
        "    logging.info(\"Aria Starting timestamp: {:0.3f}\".format(starting_timestamp))\n",
        "\n",
        "    logging.info(f\"Transcribe the speech from wav file {file_path}.\")\n",
        "    result = model.transcribe(file_path, batch_size=batch_size)\n",
        "    print(f\"Transcription done.\")\n",
        "\n",
        "\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "    logging.info(f\"Transcription done.\")\n",
        "    result_aligned = whisperx.align(\n",
        "        result[\"segments\"], model_a, metadata, file_path, device\n",
        "    )\n",
        "    print(f\"Alignment done.\")\n",
        "\n",
        "    try:\n",
        "        asr_tokens_to_csv(\n",
        "            word_segments=result_aligned[\"word_segments\"],\n",
        "            token_csv_folder=output_folder,\n",
        "            starting_timestamp_s=starting_timestamp,\n",
        "        )\n",
        "    except Exception as err:\n",
        "        logging.warning(f\"Cannot process {file_path} because {err}. Skip this recording.\")"
      ],
      "metadata": {
        "id": "_wavV43tiSwM"
      },
      "id": "_wavV43tiSwM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 14. Run Whisper X\n",
        "Finally, let's run Whisper X on the .wav file that we extracted.\n",
        "\n",
        "Make sure to\n",
        "- Change the `audio_file` to the .wav file that we extracted in Step 12.\n",
        "- Set the `whisper_x_output_folder` to desired location. The resulting file name is `speech_aria_domain.csv`."
      ],
      "metadata": {
        "id": "C3q0o21rRIbm"
      },
      "id": "C3q0o21rRIbm"
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '231-1-0000-743.444.wav'\n",
        "whisper_x_output_folder = \".\"\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type,) #  language='en'\n",
        "batch_size = 16 # reduce if low on GPU mem, or keep it None\n",
        "provider = run_whisperx_aria_wav(model, audio_file, output_folder=whisper_x_output_folder, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ZODeWa-ilKMJ"
      },
      "id": "ZODeWa-ilKMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asr_df = pd.read_csv(\"speech_aria_domain.csv\")\n",
        "display(asr_df)"
      ],
      "metadata": {
        "id": "670FaxtH1OGS",
        "outputId": "dbc76cb8-2b62-4c96-e3da-2c3d42c63731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "id": "670FaxtH1OGS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         word  startTime_ms    endTime_ms  score\n",
              "0    Awesome.  744915000000  745275000000  0.535\n",
              "1       Would  745836000000  745936000000  0.451\n",
              "2         you  745956000000  746016000000  0.958\n",
              "3        like  746056000000  746156000000  0.932\n",
              "4        some  746176000000  746296000000  0.759\n",
              "..        ...           ...           ...    ...\n",
              "222     Thank  927758000000  927978000000  0.609\n",
              "223      you.  927998000000  928159000000  0.912\n",
              "224       Big  934170000000  934330000000  0.652\n",
              "225      long  934370000000  934671000000  0.950\n",
              "226      sip.  934691000000  934931000000  0.780\n",
              "\n",
              "[227 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36794bf5-9be7-4403-a6a7-b3f62cfc8ad2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>startTime_ms</th>\n",
              "      <th>endTime_ms</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Awesome.</td>\n",
              "      <td>744915000000</td>\n",
              "      <td>745275000000</td>\n",
              "      <td>0.535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Would</td>\n",
              "      <td>745836000000</td>\n",
              "      <td>745936000000</td>\n",
              "      <td>0.451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>you</td>\n",
              "      <td>745956000000</td>\n",
              "      <td>746016000000</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>like</td>\n",
              "      <td>746056000000</td>\n",
              "      <td>746156000000</td>\n",
              "      <td>0.932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>some</td>\n",
              "      <td>746176000000</td>\n",
              "      <td>746296000000</td>\n",
              "      <td>0.759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>Thank</td>\n",
              "      <td>927758000000</td>\n",
              "      <td>927978000000</td>\n",
              "      <td>0.609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>you.</td>\n",
              "      <td>927998000000</td>\n",
              "      <td>928159000000</td>\n",
              "      <td>0.912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>Big</td>\n",
              "      <td>934170000000</td>\n",
              "      <td>934330000000</td>\n",
              "      <td>0.652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>long</td>\n",
              "      <td>934370000000</td>\n",
              "      <td>934671000000</td>\n",
              "      <td>0.950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>sip.</td>\n",
              "      <td>934691000000</td>\n",
              "      <td>934931000000</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>227 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36794bf5-9be7-4403-a6a7-b3f62cfc8ad2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36794bf5-9be7-4403-a6a7-b3f62cfc8ad2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36794bf5-9be7-4403-a6a7-b3f62cfc8ad2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-36a0f26e-0d64-4f0a-860c-987ea512536c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36a0f26e-0d64-4f0a-860c-987ea512536c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-36a0f26e-0d64-4f0a-860c-987ea512536c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional steps for summarization example\n",
        "Proceed Step 15-17, if you would like to try out summarization of the narration using llm (via langchain)."
      ],
      "metadata": {
        "id": "ihTkYWNb4BAG"
      },
      "id": "ihTkYWNb4BAG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15. Install Langchain"
      ],
      "metadata": {
        "id": "SBANXKSPR1eP"
      },
      "id": "SBANXKSPR1eP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "t25suH5HR5jb"
      },
      "id": "t25suH5HR5jb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16. Install OpenAI to use with Langchain"
      ],
      "metadata": {
        "id": "temVxjVVWBw3"
      },
      "id": "temVxjVVWBw3"
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "r6QEuPO0WFgC"
      },
      "id": "r6QEuPO0WFgC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 17. Summaraize the narration result\n"
      ],
      "metadata": {
        "id": "Nb6m7nHzSBy9"
      },
      "id": "Nb6m7nHzSBy9"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "prompt_template = \"\"\" Write a concise summary (between 5 to 10 sentences) of the following text.\n",
        "The text is about my exhaustive timeline, where I am referred to as '#C C' or 'C C' or 'C'.\n",
        "Please use first person pronoun (I) in the summary, instead of 'C' or 'C C'.\n",
        "Please keep in mind that some observations maybe incorrect as the timeline was machine-generated.\n",
        "\n",
        "Timeline:\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "TL'DR: \"\"\"\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key\"\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "loader = CSVLoader(file_path=narration_save_path)\n",
        "docs = loader.load()\n",
        "\n",
        "chain.run(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "igRz39kKSBES",
        "outputId": "c4f09b38-7d0e-42a1-c451-a50c88f250c7"
      },
      "id": "igRz39kKSBES",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The summary describes a series of events with timestamps and corresponding narrations. The events involve a character named C C who is observed looking around, adjusting the camera, staring at the ceiling, standing beside the door, and looking at various objects in the room and house.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) RGBDataset\n",
        "\n",
        "The RGBDataset is a simple image pytorch dataset designed for image-based models that operate on individual frames rather than snippet inputs. Use this dataset that process single frames."
      ],
      "metadata": {
        "id": "oV6Tn_WACKMF"
      },
      "id": "oV6Tn_WACKMF"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# class RGBDataset(Dataset):\n",
        "#     def __init__(self, start_time, end_time, sample_count, transform=None):\n",
        "#         self.timestamps = np.linspace(start_time, end_time, sample_count)\n",
        "#         self.rgb_stream_id = StreamId(\"214-1\")\n",
        "#         self.time_domain = TimeDomain.DEVICE_TIME\n",
        "#         self.option = TimeQueryOptions.CLOSEST\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.timestamps)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         timestamp = self.timestamps[idx]\n",
        "#         image_tuple = provider.get_image_data_by_time_ns(self.rgb_stream_id, int(timestamp), self.time_domain, self.option)\n",
        "#         image_array = image_tuple[0].to_numpy_array()\n",
        "#         image = Image.fromarray(image_array).rotate(-90)\n",
        "#         if self.transform:\n",
        "#           image = self.transform(image)\n",
        "#         return timestamp, image\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.ToTensor(),\n",
        "#   ])\n",
        "\n",
        "# rgb_dataset = RGBDataset(start_time, end_time, sample_count, transform=val_transform)\n",
        "# image_dataloader = DataLoader(rgb_dataset, batch_size=2, shuffle=False)\n",
        "# # Get the next batch of data\n",
        "# timestamp, image = next(iter(image_dataloader))"
      ],
      "metadata": {
        "id": "kvAqkUzdCLPt"
      },
      "id": "kvAqkUzdCLPt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "custom": {
      "cells": [],
      "metadata": {
        "fileHeader": "",
        "kernelspec": {
          "display_name": "Python 3 (ipykernel)",
          "language": "python",
          "name": "python3"
        },
        "language_info": {
          "codemirror_mode": {
            "name": "ipython",
            "version": 3
          },
          "file_extension": ".py",
          "mimetype": "text/x-python",
          "name": "python",
          "nbconvert_exporter": "python",
          "pygments_lexer": "ipython3",
          "version": "3.10.11"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 5
    },
    "indentAmount": 2,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}